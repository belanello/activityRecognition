---
title: "Qualitative Activity Recognition of Weight Lifting Exercises"
author: "Ayako Nagao"
date: "10/31/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message=FALSE)
```

### 1.Overview  
The Qualitative Activity Recognition is to recognize how well the certain activities are performed. In this project, we use the data [Weight Lifting Exercises Dataset](http://web.archive.org/web/20161217164008/http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201), which was collected by wearable sensors while participants are performing the weight lifting exercises in correct ways and also with a set of common mistakes. The goal of this project is to build the model to classify the exercise categories described below.

- Exactly according to the specification (Class A)  
- Throwing the elbows to the front (Class B)  
- Lifting the dumbbell only halfway (Class C)  
- Lowering the dumbbell only halfway (Class D) 
- Throwing the hips to the front (Class E)  

### 2.Data  

#### 2.1  Load the data  

```{r loadData,cache=TRUE}
url <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
data <- read.csv(url)
dim(data)
```
#### 2.2 Split the dataset  

We use 70% of data for trainig the model, 30% for testing the model accuracy.
```{r splitData, cache=TRUE}
library(caret)
set.seed(10103)
inTrain <- createDataPartition(y=data$classe,p=0.7,list=FALSE)
train <- data[inTrain,]
test <- data[-inTrain,]
obs <- dim(train)[1]; var <- dim(train)[2]
```
#### 2.3 Data cleaning  

The training dataset contains `r obs` observations and `r var` features. In this dataset there are many features with NA or empty strings. All features with NA, around 98% of the observations are NA(missing), all features with empty strings, also around 98% of the observations are empty strings(missing). Thus we omit those features and also omit features irrelevant to activity recognition such as user names, time stamp, and new window.


```{r summaryTraining, cache=TRUE}
# select only numeric variables(character vars are 98% empty strings)
dataClass <- sapply(train,class)
numVar <- names(dataClass[dataClass!='character'])
train <- train[,c(numVar,'classe')]
# remove irrelevant variables
irrelevant <- match(c('X','raw_timestamp_part_1','raw_timestamp_part_2','num_window'),names(train))
train <- train[,-irrelevant]
dim(train)

# count NAs for each column and find feature names for non-NA
NAcounts <- colSums(is.na(train))
nonNaVar <- names(NAcounts[NAcounts==0])
train <- train[,nonNaVar]
obs2 <- dim(train)[1]; var2 <- dim(train)[2]
```
After Data cleaning, training dataset has `r obs2` and `r var2` features including the target label.

### 3.Model and feature selection  
#### 3.1 Model selection  
Since many of the features are highly correlated each other, I chose the Random Forest model in which collinearity is not a issue. 

#### 3.2 Feature selection  
The approach I take is  

1. Fit the Random Forest model with all the features and calculate the feature importance(mean decrease of gini index). 
2. Fit the model removing feature from the least important one and continue till 39 features are removed.
3. Calculate the test set accuracy for each model with reduced features.
4. Repeat previous steps 2 and 3 for 8 different folds.  
5. Plot the test accuracy for the 8 different folds along with the mean values.
6. Decide how many features we omit or model with all the features. 


```{r featureExtraction, cache=TRUE}
# split  the training data for another temporary training set for feature selection
set.seed(1234)
inTempTrain <- createDataPartition(y=train$classe,
                                   p=0.7,
                                   list=FALSE)
tempTrain <- train[inTempTrain,]
tempTest <- train[-inTempTrain,]
tempTrain$classe <- factor(tempTrain$classe)
tempTest$classe <- factor(tempTest$classe)

library(randomForest)
set.seed(1892)
mdl1 <- randomForest(classe ~.,data=tempTrain,importance=TRUE)
# sort feature importance ascending order
varImp <-sort(mdl1$importance[,7])
varImpDf <- data.frame(idx=1:52,
                       featureNames=names(varImp),
                       meanDecreaseGini=as.numeric(varImp))
library(ggplot2)
g <- ggplot(varImpDf,aes(x=meanDecreaseGini,y=idx))
g <- g + geom_bar(stat='identity',orientation='y')
g <- g + scale_y_continuous(breaks=1:52,labels=varImpDf$featureNames)
g <- g + labs(title='Fig.1:Feature Importance (Mean decrease of gini index)')
g + ylab('')

```

```{r featureExtraction2, cache=TRUE}
# create 8 folds for testing out of temporary training data
set.seed(1324)
folds <- createFolds(y=tempTrain$classe,k=8,returnTrain=TRUE)
# create dataframe for storing the accuracy of each model
accDf <- data.frame(matrix(rep(0,320),nrow=8))
for (i in 1:8){
  trainFold <- tempTrain[folds[[i]],]
  testFold <- tempTrain[-folds[[i]],]

  for (j in 1:40){
    vars <- varImpDf$featureNames[j:52]
    set.seed(1357)
    m <- randomForest(classe ~.,data=trainFold[,c(vars,'classe')],
                      xtest=testFold[,vars],
                      ytest=testFold[,'classe'])
    accDf[i,j] <- mean(m$test$predicted==testFold$classe)
  }
}

# transpose accDf for plotting
accDfT <- data.frame(cbind(seq(1,40),t(accDf)))
colnames(accDfT) <- c('idx','Fold1','Fold2','Fold3','Fold4','Fold5',
                'Fold6','Fold7','Fold8')
# make accDfT long format
library(reshape2)
accDfLong <- melt(accDfT,id.vars='idx')
# make mean accuracy dataframe 

accDfMean <- data.frame(idx=1:40,means=colMeans(accDf),
                        variable=rep('mean',40))

# plot the results
g <- ggplot(accDfLong,aes(x=idx,y=value,group=variable))
g <- g + geom_point(col='steelblue')+geom_line(col='steelblue')
g <- g + geom_point(data=accDfMean,
                    aes(x=idx,y=means),
                    col='red') + geom_line(data=accDfMean,
                                           aes(x=idx,y=means),
                                           col='red')
g <- g + scale_x_continuous(breaks=seq(1,40,by=2),
                          labels=seq(52,13)[rep(c(TRUE,FALSE),20)])
g <- g + labs(title='Fig.2:Test set accuracy of the reduced number of features')
g <- g + xlab('The number of features') + ylab('Accuracy')
g


```
  
In Figure.2, the test set accuracy of 8 different folds with all 52 features From the far left which reduced one by one till 13 features are plotted in blue line. The red line is average accuracy of 8 different folds. It is surprising to see we could predict 97.7 % of responses on average with only the most important 13 features. We can see from the model with 25 features, the average accuracy starts to go down noticeably. Thus we omit 26 features keep the rest of 26 more important features. To make sure that we get the same level of accuracy, I fit the model with new subset of features and test with the data that I set aside.

```{r testNumberOfFeatures,cache=TRUE}
vars26 <- varImpDf$featureNames[27:52]
set.seed(1084)
mdl2 <- randomForest(classe ~.,data=tempTrain[,c(vars26,'classe')],
                    xtest=tempTest[,vars26],
                    ytest=tempTest[,'classe'])
accMdl2 <- round(mean(mdl2$test$predicted==tempTest$classe),4)

```
With 26 features above, we got `r accMdl2` accuracy.  

### 4.Modeling  

Now we train the model with the selected features using default  setting of train function for random forest model in caret package.  

```{r modeling,cache=TRUE}

# subset the training and test set for the selected features
train <- train[,c(vars26,'classe')]
test <- test[,c(vars26,'classe')]

# train the model 
set.seed(8563) 
mdl3 <- train(classe ~.,data=train,method='rf')
mdl3
```
### 5.Results  
```{r results}
print(paste('training set accuracy: ',mean(predict(mdl3)==train$classe)))
print(paste('test set accuracy: ',mean(predict(mdl3,test)==test$classe)))

table(predict(mdl3,test),test$classe)
```